---
output:
  word_document: 
    fig_height: 6
    fig_width: 8
    reference_docx: "../template/WordTemplate_EasyR_Text.docx"
    toc: true
    toc_depth: 2
toc-title: "목차"
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::knit_child("../rmd_etc/rmd_options.Rmd")

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()
```


# 2. 형태소 분석기를 이용한 단어 빈도 분석


1장에서는 텍스트를 띄어쓰기 기준으로 토큰화하여 단어 빈도를 분석했습니다. 그런데 띄어쓰기 기준으로 토근화하면 '합니다', '있습니다'와 같이 의미를 지니지 않는 서술어가 가장 많이 추출되어서 빈도 분석을 해도 텍스트가 무엇을 강조는지 알기 어렵습니다. 그래서 토큰화는 띄어쓰기가 아닌 의미 단위를 기준으로 하는 것이 좋습니다. 이번 장에서는 텍스트를 의미 단위로 토큰화해 분석하는 방법을 알아보겠습니다.


## 2.1 형태소 분석


의미를 가진 가장 작은 말의 단위를 '형태소'라 합니다. 그리고 문장에서 형태소를 추출해 명사, 동사, 형용사와 같은 품사로 분류하는 작업을 **형태소 분석(Morphological Analysis)**이라고 합니다. 한글을 토큰화할 때는 띄어쓰기가 아닌 형태소를 기준으로 삼는 것이 일반적입니다. 특히 형태소 중에도 명사를 보면 텍스트가 무엇에 관한 내용인지 쉽게 파악할 수 있기 때문에 텍스트에서 명사만 추출해 토큰으로 삼는 방법을 많이 사용합니다.


### 2.1.1 형태소 분석 패키지 설치하기

`KoNLP`(Korean Natural Language Processing) 패키지를 이용하면 한글 텍스트의 형태소를 분석할 수 있습니다. `KoNLP` 패키지를 설치해 보겠습니다.


#### 1. 자바와 rJava 패키지 설치하기

`KoNLP` 패키지는 자바와 `rJava` 패키지가 설치되어 있어야 사용할 수 있습니다. 자바와 `rJava` 패키지는 `multilinguer` 패키지의 `install_jdk()`를 이용해 설치할 수 있습니다.

```{r, eval = F}
install.packages("multilinguer")
library(multilinguer)
install_jdk()
```


#### 2. KoNLP 의존성 패키지 설치하기

어떤 패키지는 다른 패키지의 기능을 활용하도록 만들어져 있기 때문에 여러 패키지를 함께 설치해야만 정상적으로 작동합니다. 이처럼 패키지가 의존하고 있는 패키지를 의존성 패키지라고 합니다. `KoNLP` 패키지를 사용하려면 다음의 의존성 패키지들을 먼저 설치해야 합니다.

```{r, eval = F}
install.packages(c("stringr", "hash", "tau", "Sejong", "RSQLite", "devtools"),
                 type = "binary")
```


#### 3. KoNLP 패키지 설치하기
`remotes` 패키지의 `install_github()`를 이용해 깃허브에 업로드된 `KoNLP` 패키지를 설치한 후 로드하겠습니다.
```{r, eval=F}
install.packages("remotes")
remotes::install_github("haven-jeon/KoNLP",
                        upgrade = "never",
                        INSTALL_opts = c("--no-multiarch"))

library(KoNLP)
```

> [참고] `KoNLP`패키지가 이미 설치되어 있으면 `...has not changed since last install` 이라는 메시지가 출력됩니다. 이런 경우 `install_github()`에 `force = T`를 추가하면 패키지를 강제로 다시 설치합니다.

#### 4. 형태소 사전 설정하기

`KoNLP` 패키지가 지원하는 'NIA 사전'은 120만여 개 단어로 구성되어 있습니다. 형태소 분석을 하는 데 이 사전을 이용하도록 `useNIADic()`을 실행하겠습니다.


```{r, eval = F}
useNIADic()
```


> [참고] `useNIADic()`은 `KoNLP` 패키지 설치 후 한 번만 실행하면 됩니다.


### 2.1.2 형태소 분석기를 이용해 토큰화하기

`KoNLP` 패키지를 이용할 준비가 되었습니다. 텍스트를 형태소 분석기로 토큰화하여 명사를 추출하는 방법을 알아보겠습니다.

#### 명사 추출하기 - `extractNoun()`

명사를 보면 텍스트가 무엇에 관한 내용인지 쉽게 파악할 수 있기 때문에 텍스트에서 명사를 추출해 빈도를 분석하는 경우가 많습니다. `KoNLP` 패키지의 `extractNoun()`은 텍스트의 형태소를 분석해 명사를 추출합니다. 샘플 텍스트에 `extractNoun()`을 적용한 결과를 보면 각 문장에서 명사만 추출하여 list 구조로 출력했음을 알 수 있습니다.

```{r echo=T}
library(KoNLP)
```


```{r}
library(dplyr)
text <- tibble(
  value = c("대한민국은 민주공화국이다.",
            "대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다."))
text

extractNoun(text$value)
```


> [참고] extractNoun()은 처음 실행하면 시간이 오래 걸리지만 두 번째 실행부터는 빠르게 작동합니다.

#### `unnest_tokens()`에 `extractNoun`을 적용하여 명사 추출하기

`unnest_tokens()`의 `token`에 `extractNoun`을 입력해도 명사 기준으로 텍스트를 토큰화할 수 있습니다. `unnest_tokens()`는 다루기 쉬운 tibble 구조로 결과를 출력하는 장점이 있기 때문에 토큰화할 때 활용하면 편리합니다. 다음 코드를 실행하면 텍스트에서 명사만 추출해 tibble 구조로 출력됐음을 확인할 수 있습니다.

```{r}
library(tidytext)
text %>%
  unnest_tokens(input = value,        # 분석 대상
                output = word,        # 출력 변수명
                token = extractNoun)  # 토큰화 함수
```


> [참고] `token` 파라미터에 입력한 `extractNoun`은 문자열이 아니라 함수명이기 때문에  앞뒤에 따옴표를 입력하지 않으니 주의하세요.

#### 연설문에서 명사 추출하기

형태소 분석기를 이용해 토큰화하는 방법을 익혔으니 문재인 대통령 연설문에 적용해보겠습니다. 우선 연설문을 불러와 기본적인 전처리를 한 후에 명사 기준으로 토큰화하겠습니다. 다음 코드를 실행하면 연설문에서 명사만 추출돼 각 행으로 구성된 것을 확인할 수 있습니다.

<!-- 원고 코드 -->
```{r eval=FALSE}
# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8")

# 기본적인 전처리
library(stringr)
library(textclean)

moon <- raw_moon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 중복 공백 제거
  as_tibble()                           # tibble로 변환

# 명사 기준 토큰화
word_noun <- moon %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

word_noun
```


<!-- 실행 코드 -->
```{r echo=F}
# 문재인 대통령 연설문 불러오기
raw_moon <- readLines(here::here("files/speech_moon.txt"), encoding = "UTF-8")


# 기본적인 전처리
library(stringr)
library(textclean)

moon <- raw_moon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 중복된 공백 제거
  as_tibble()                           # tibble로 변환

# 명사 기준 토큰화
word_noun <- moon %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

word_noun
```


## 2.2 명사 빈도 분석하기

연설문에서 명사를 추출했으니 이제 빈도를 구할 차례입니다. 1장과 마찬가지로 두 글자 이상 단어만 추출해 빈도를 구하겠습니다.

### 2.2.1 단어 빈도 구하기

다음 코드의 출력 결과를 보면 연설문에서 어떤 단어가 많이 사용됐는지 알 수 있습니다. 명사를 기준으로 토큰화했기 때문에 띄어쓰기 기준으로 토큰화했던 1장의 분석 결과보다 텍스트에 어떤 주제가 강조되었는지 쉽게 알 수 있습니다. `# A tibble: 704 x 2`를 보면 연설문이 704개의 명사로 구성되어 있음을 알 수 있습니다.

```{r}
word_noun <- word_noun %>%
  count(word, sort = T) %>%    # 단어 빈도 구해 내림차순 정렬
  filter(str_count(word) > 1)  # 두 글자 이상만 남기기

word_noun
```



### 2.2.2 막대 그래프 만들기

어떤 단어가 자주 사용됐는지 쉽게 알아볼 수 있도록 빈도가 가장 높은 상위 20개 단어를 추출해 막대 그래프를 만들겠습니다.

```{r}
# 상위 20개 단어 추출
top20 <- word_noun %>%
  head(20)

# 막대 그래프 만들기
library(ggplot2)
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```


### 2.2.3 워드 클라우드 만들기

이번에는 워드 클라우드를 만들어 보겠습니다. 출력한 워드 클라우드를 보면 명사를 추출해 만들었기 때문에 1장에서 만든 워드 클라우드보다 연설문이 어떤 내용을 다루고 있는지 이해하기 쉽습니다.

```{r}
# 폰트 설정
library(showtext)
font_add_google(name = "Black Han Sans", family = "blackhansans")
showtext_auto()

library(ggwordcloud)
ggplot(word_noun, aes(label = word, size = n, col = n)) +
  geom_text_wordcloud(seed = 1234, family = "blackhansans") +
  scale_radius(limits = c(3, NA),
               range = c(3, 15)) +
  scale_color_gradient(low = "#66aaf2", high = "#004EA1") +
  theme_minimal()
```


## 2.3 특정 단어가 사용된 문장 살펴보기

가장 많이 사용된 단어가 무엇인지 알았으니, 이제 이 단어가 사용된 문장을 직접 확인해 보겠습니다. 문장을 읽어보면 글쓴이가 구체적으로 어떤 목적을 가지고 단어를 사용했는지 이해할 수 있습니다.


### 2.3.1 문장 기준으로 토큰화하기

우선, 전처리를 하지 않은 원문 `raw_moon`을 문장 기준으로 토큰화 하겠습니다. 명사 기준으로 토큰화 할 때는 `unnest_tokens()`의 `token`에 `extractNoun`을 입력했습니다. 문장 기준으로 토큰화하려면 `token`에 `"sentences"`를 입력하면 됩니다. 출력 결과를 보면 각 행이 문장으로 구성되어 있습니다.

```{r}
sentences_moon <- raw_moon %>%
  str_squish() %>%
  as_tibble() %>%
  unnest_tokens(input = value,
                output = sentence,
                token = "sentences")

sentences_moon
```


> [참고] 문장으로 토큰화할 때는 마침표가 문장의 기준점이므로 특수문자를 제거하는 전처리를 거치면 안 됩니다. 1장에서는 특수문자를 제거하기 위해 `str_replace_all("[^가-힣]", " ")`를 사용했지만 여기서는 사용하지 않습니다.


### 2.3.2 특정 단어가 사용된 문장 추출하기

문장 기준으로 토큰화했으니 이제 특정 단어가 사용된 문장을 추출해 확인하겠습니다.


#### 특정 단어가 들어 있는지 확인하기 - `str_detect()`

`stringr` 패키지의 `str_detect()`를 이용하면 문장에 특정 단어가 들어 있는지 확인할 수 있습니다. `str_detect()`는 지정한 단어가 문장에 있으면 `TRUE`, 그렇지 않으면 `FALSE`를 반환합니다.


```{r}
str_detect("치킨은 맛있다", "치킨")
str_detect("치킨은 맛있다", "피자")
```


#### 특정 단어가 사용된 문장 추출하기

이제 `str_detect()`을 이용해 연설문에서 특정 단어가 사용된 문장을 추출하겠습니다. 앞에서 `"국민"`, `"일자리"`가 21번 등장해 가장 많이 사용된 단어임을 확인했습니다. 이 단어가 사용된 문장을 추출하겠습니다. `filter()`에 `str_detect()`를 적용해 추출할 단어를 입력하면 됩니다. 출력된 문장을 보면 두 단어가 어떤 용도로 사용되었는지 이해할 수 있습니다.

```{r eval=F}
sentences_moon %>%
  filter(str_detect(sentence, "국민"))
```

```{r echo=F}
sentences_moon %>%
  filter(str_detect(sentence, "국민")) %>% print(n = 10)
```

<br>

```{r eval=F}
sentences_moon %>%
  filter(str_detect(sentence, "일자리"))
```

```{r echo=F}
sentences_moon %>%
  filter(str_detect(sentence, "일자리")) %>% print(n = 10)
```

> [참고] tibble 구조는 텍스트가 길면 콘솔 창 크기에 따라 일부만 출력됩니다. 모든 내용을 출력하려면 코드 뒤에 `%>% data.frame()`를 추가해 데이터 프레임으로 변환하면 됩니다. 왼쪽 정렬해 출력하려면 `%>% print.data.frame(right = F)`를 추가하면 됩니다.

<br>

> [알아두면 좋아요] 형태소 분석기는 한계가 있습니다.

> 형태소 분석기를 이용하다 보면 분석 결과에 "하게"와 같이 의미를 알 수 없는 단어가 포함될 때가 있습니다. 이는 형태소 분석기가 "당당하게", "절실하게"와 같은 단어의 "하게"를 명사로 분류해 생긴 오류입니다. 형태소 분석기가 사용하는 사전에 "하게"라는 명사가 있어서 오류가 생긴 것입니다. 이런 한계가 있기 때문에 분석하면서 오류를 찾아 수정해야 합니다. 이 과정은 뒤에서 자세히 다룹니다.

