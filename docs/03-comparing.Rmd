---
title: "Do it! 쉽게 배우는 R 텍스트 마이닝 - 03 비교 분석: 무엇이 다를까?"
author: "김영우"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "css/custom.css"]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:10'
      navigation:
        scroll: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, 
        width = 70,
        max.print = 80,
        tibble.print_max = 40,
        tibble.width = 70,
        servr.interval = 0.01) # Viewer 수정 반영 속도

knitr::opts_chunk$set(cache = T, warning = F, message = F, 
                      dpi = 300, fig.height = 4, out.width = "100%")

xaringanExtra::use_tile_view()

library(knitr)
library(icon)
library(here)
```


```{r echo=FALSE}
rm(list = ls())

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()

# code highlighting
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})


```

class: title0

Do it! 쉽게 배우는 R 텍스트 마이닝

---

<br>

.pull-left[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
```{r, echo=FALSE, out.width="70%", out.height="70%"}
include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/cover.png")
```
]

.pull-right[

<br>
<br>
<br>

`r fontawesome("github")` [github.com/youngwoos/Doit_textmining](https://github.com/youngwoos/Doit_textmining)

`r fontawesome("facebook-square")` [facebook.com/groups/datacommunity](https://facebook.com/groups/datacommunity)

- [네이버책](https://book.naver.com/bookdb/book_detail.nhn?bid=17891971)
  - [yes24](http://bit.ly/3oUuJOB)
  - [알라딘](http://bit.ly/3oXOSDn)
  - [교보문고](https://bit.ly/2LtNOcB)
]

---

class: title0

03 비교 분석: 무엇이 다를까?

---

class: title0-2

We'll make

<br-back-10>

```{r, echo=FALSE, out.width="60%", out.height="60%"}
include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/02/01_2_1.png")
```

---

class: title0-2

and

<br-back-40>

```{r, echo=F, out.width="60%", out.height="60%"}
# include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/02/01_2_2.png")
include_graphics("../Image/02/01_2_2.png")
```

---

<br>

.large2[.font-jua[목차]]

.large[.font-jua[03-1 단어 빈도 비교하기]]([link](#03-1))

.large[.font-jua[03-2 오즈비 - 상대적으로 중요한 단어 비교하기]]([link](#03-2))

.large[.font-jua[03-3 로그 오즈비로 단어 비교하기]]([link](#03-3))

.large[.font-jua[03-4 TF-IDF - 여러 텍스트의 단어 비교하기]]([link](#03-4))

---


name: 03-1
class: title1

03-1 단어 빈도 비교하기

---

### 비교 분석
- 여러 텍스트를 비교해 차이를 알아보는 분석 방법
- 단어 빈도 분석을 응용해 자주 사용된 단어의 차이를 살펴봄

---

### 텍스트 합치기

- 텍스트를 비교하기 위해 여러 개의 텍스트를 하나의 데이터셋으로 합치는 작업

--

##### 데이터 불러오기

- 문재인 대통령과 박근혜 전 대통령의 대선 출마 선언문 불러오기
- tibble 구조로 변환하고 연설문 구분 위해 대통령 이름 부여

```{r eval=FALSE}
library(dplyr)

# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8")
moon <- raw_moon %>%
  as_tibble() %>%
  mutate(president = "moon")

# 박근혜 대통령 연설문 불러오기
raw_park <- readLines("speech_park.txt", encoding = "UTF-8")
park <- raw_park %>%
  as_tibble() %>%
  mutate(president = "park")
```

```{r echo=F}
library(dplyr)

# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("../Data/speech_moon.txt", encoding = "UTF-8")

moon <- raw_moon %>%
  as_tibble() %>%
  mutate(president = "moon")

# 박근혜 대통령 연설문 불러오기
raw_park <- readLines("../Data/speech_park.txt", encoding = "UTF-8")

park <- raw_park %>%
  as_tibble() %>%
  mutate(president = "park")
```

---

##### 데이터 합치기

- 두 데이터를 행(세로) 방향으로 결합
- 출력 결과 보기 편하게 `select()`로 변수 순서 바꾸기


```{r}
bind_speeches <- bind_rows(moon, park) %>%
  select(president, value)
```

---

- `bind_speeches` 윗부분은 문재인 대통령, 아랫부분은 박근혜 전 대통령 연설문

.pull-left[

```{r}
head(bind_speeches)
```
]

.pull-right[

```{r}
tail(bind_speeches)
```
]

`r fontawesome("lightbulb")` 박근혜 전 대통령의 대선 출마 선언문 출처: [bit.ly/easytext_31](https://bit.ly/easytext_31)
 
 
---


#### 집단별 단어 빈도 구하기

##### 1. 기본적인 전처리 및 토큰화

- 한글 이외의 문자, 연속된 공백 제거

```{r}
# 기본적인 전처리
library(stringr)
speeches <- bind_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))
```

`r icon_style(fontawesome("exclamation-triangle"), fill = "#FF7333")` `bind_speeches`는 tibble 구조이므로 `mutate()` 활용


---

```{r}
speeches
```

---

- 형태소 분석기를 이용해 명사 기준 토큰화

.scroll-box-26[

```{r}
# 토큰화
library(tidytext)
library(KoNLP)

speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

speeches
```

]

---



#### 하위 집단별 단어 빈도 구하기 - `count()`

- `"moon"`과 `"park"`의 단어 빈도 각각 구하기

--

##### 샘플 텍스트로 작동 원리 알아보기

- `count()`에 집단을 구성하는 두 변수를 순서대로 입력

```{r}
df <- tibble(class = c("a", "a", "a", "b", "b", "b"),
             sex = c("female", "male", "female", "male", "male", "female"))
```

.pull-left[

```{r}
df
```
]


.pull-right[

```{r}
df %>% count(class, sex)
```
]

---
   

##### 두 연설문의 단어 빈도 구하기

```{r}
frequency <- speeches %>%
  count(president, word) %>%   # 연설문 및 단어별 빈도
  filter(str_count(word) > 1)  # 두 글자 이상 추출

head(frequency)
```

`r fontawesome("lightbulb")`  `count()`는 입력한 변수의 알파벳, 가나다순으로 행을 정렬함

---


#### 자주 사용된 단어 추출하기

- `dplyr::slice_max()`: 값이 큰 상위 n개의 행을 추출해 내림차순 정렬

--

##### 샘플 데이터로 작동 원리 알아보기


```{r}
df <- tibble(x = c(1:100))
```

<br-back-20>

.pull-left[

```{r}
df
```
]

.pull-right[
```{r}
df %>% slice_max(x, n = 3)
```



]

---

.pull-left[

`r fontawesome("lightbulb")`  `slice_min()`: 값이 작은 하위 n개 추출
```{r}
df %>% slice_min(x, n = 3)
```
]


---

#### 연설문에 가장 많이 사용된 단어 추출하기

- `president`별 고빈도 단어 상위 10개 추출

<br-back-20>

.scroll-box-24[

```{r}
top10 <- frequency %>%
  group_by(president) %>%  # president별로 분리
  slice_max(n, n = 10)     # 상위 10개 추출

top10
```

]
---

#### 단어 빈도 동점 처리

- `# A tibble: 22 x 3`: 두 연설문에서 단어 10개씩 추출했는데 20행이 아니라 22행
- 단어 빈도 동점인 행이 전부 추출되었기 때문
  
---
  
- 박근혜 전 대통령의 연설문 단어 12개
  - `"교육"`, `"사람"`, `"사회"`, `"일자리"` 빈도 동점, 모두 추출되면서 행 늘어남

.pull-left[
```{r, highlight.output = c(13:16)}
top10 %>%
  filter(president == "park")
```
]

---


#### 빈도 동점 단어 제외하고 추출하기 
- `slice_max(with_ties = F)`: 원본 데이터의 정렬 순서에 따라 행 추출

##### 샘플 데이터로 작동 원리 알아보기

```{r}
df <- tibble(x = c("A", "B", "C", "D"), y = c(4, 3, 2, 2))
```

<br-back-20>

.pull-left[

```{r}
df %>%
  slice_max(y, n = 3)
```
]

.pull-right[
```{r}
df %>%
  slice_max(y, n = 3, with_ties = F)
```
]
---

#### 연설문에 적용하기


.scroll-box-26[

```{r, highlight.output=1}
top10 <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)

top10

```
]

---

#### 막대 그래프 만들기

##### 1. 변수의 항목별로 그래프만들기 - `facet_wrap()`
- `~` 뒤에 그래프를 나누는 기준 변수 입력

```{r eval=F}
library(ggplot2)
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president)
```

---

```{r echo=F}
library(ggplot2)
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president)
```


---


- 축을 구성하는 단어가 한 범주에만 있으면 축은 있지만 막대는 없는 항목 생김
  - ex) `"행복"`: `park`, `"나라"`: `moon`
      
---
  

##### 2. 그래프별 y축 설정하기

- `scales`: 그래프의 축 통일 또는 각각 생성 결정
  - `"fixed"`: 축 통일(기본값)
  - `"free_y"`: 범주별로 y축 만듦

```{r eval=F}
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president,         # president별 그래프 생성
              scales = "free_y")  # y축 통일하지 않음
```

---

```{r echo=F}
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president,         # president별 그래프 생성
              scales = "free_y")  # y축 통일하지 않음
```


---


#### 3. 특정 단어 제외하고 막대 그래프 만들기

- 박근혜 전 대통령 `"국민"` 빈도 너무 높아 다른 단어들 차이 드러나지 않음
- 전반적인 단어 빈도가 잘 드러나도록 제거

.scroll-box-20[

```{r}
top10 <- frequency %>%
  filter(word != "국민") %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)

top10
```
]

---

```{r eval=F}
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")
```

---

```{r echo=F}
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")
```


---


#### 4. 축 정렬하기

- x축을 지정할 때 `reorder()`를 사용했는데도 막대가 빈도 기준으로 완벽하게 정렬되지 않음
- 전체 빈도 기준으로 각 범주의 x축 순서를 정했기 때문

--

##### 그래프별로 축 정렬하기

- `tidytext::reorder_within()`: 변수의 항목별로 축 순서 따로 구하기
  - `x` : 축
  - `by` :정렬 기준
  - `within` : 그래프를 나누는 기준
  
```{r eval=F}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")
```

---
  
```{r echo=F}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")
```

---

#### 5. 변수 항목 제거하기

- `tidytext::scale_x_reordered()`: 각 단어 뒤의 범주 항목 제거

```{r eval=FALSE}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트
```

---

```{r echo=FALSE}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트
```

---
name: 03-2
class: title1

03-2 오즈비 - 
<br>
상대적으로  중요한 단어 비교하기

---

##### 빈도 높은 단어를 비교하면
- 어떤 텍스트든 일반적인 단어 빈도 높아 텍스트 차이 잘 드러나지 않음
  - ex) 연설문: `"우리"`, `"사회"`, `"경제"`, `"일자리"`
--

- 특정 텍스트에는 많이 사용되었지만 다른 텍스트에는 적게 사용된 단어를 살펴봐야함

---


#### Long form을 Wide form으로 변환하기

- 여러 텍스트 비교하기 편하게 데이터 구조 바꾸기

- `frequency`: `president`가 `"moon"`인 행과 `"park"`인 행이 세로로 길게 나열된 **long form**

--

##### Long form 데이터 살펴보기

```{r}
df_long <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10) %>%
  filter(word %in% c("국민", "우리", "정치", "행복"))
```

---

.pull-left[

```{r}
df_long
```

]

<br>

.pull-right[

- 같은 단어가 범주별로 다른 행을 구성
  - 범주별 빈도 비교 어려움
  - 빈도를 활용해 연산하기 불편함
]
---


#### Long form을 Wide form으로 변형하기

- **wide form**: 가로로 넓은 형태의 데이터
  - 범주별로 단어 빈도 비교하기 편함
  - 변수간 연산하기 편함


--


- `tidyr::pivot_wider()`: long form을 wide form으로 변환
  - `names_from`: 변수명으로 만들 값이 들어 있는 변수
  - `values_from`: 변수에 채워넣을 값이 들어 있는 변수
  
```{r eval=F}
install.packages("tidyr")
library(tidyr)

df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n)

df_wide
```

```{r echo=F}
# install.packages("tidyr")
library(tidyr)

df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n)
```

---

```{r}
df_wide
```

- 한 단어가 한 행으로 구성됨
- 범주별 단어 빈도 비교하기 쉬움  


---


##### `NA`를 `0`으로 바꾸기

- 어떤 단어가 둘 중 한 범주에만 있으면 `NA`
- 오즈비 계산하기 위해 `0`으로 변환해야 함

```{r}
df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0)) #<<

df_wide
```


---

##### long form, wide form 비교하기


.pull-left[

```{r}
df_long
```
]

.pull-right[

```{r}
df_wide
```
]

---

##### 연설문 단어 빈도를 Wide form으로 변환하기


```{r}
frequency_wide <- frequency %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0))

frequency_wide
```


---


#### 오즈비 구하기

- **오즈비(odds ratio)**
  - 어떤 사건이 A 조건에서 발생할 확률이 B 조건에서 발생할 확률에 비해 얼마나 더 큰지를 나타낸 값
  - 단어가 두 텍스트 중 어디에 등장할 확률이 높은지, 상대적인 중요도를 알 수 있음

---


##### 1. 단어의 비중을 나타낸 변수 추가하기

- 각 단어가 두 연설문에서 차지하는 비중을 나타낸 변수 추가
- 연설문별로 '각 단어의 빈도'를 '모든 단어 빈도의 합'으로 나눔

```{r}
frequency_wide <- frequency_wide %>% 
  mutate(ratio_moon = ((moon)/(sum(moon))),  # moon 에서 단어의 비중
         ratio_park = ((park)/(sum(park))))  # park 에서 단어의 비중

frequency_wide

```

---

##### 어떤 단어가 한 연설문에 전혀 사용되지 않으면
- 빈도 0, 오즈비 0. 단어 비중 비교할 수 없음
- 빈도가 0보다 큰 값이 되도록 모든 값에 `+1`

```{r}
frequency_wide <- frequency_wide %>%
  mutate(ratio_moon  = ((moon + 1)/(sum(moon + 1))),  # moon에서 단어의 비중
         ratio_park  = ((park + 1)/(sum(park + 1))))  # park에서 단어의 비중

frequency_wide
```

---

#### 2. 오즈비 변수 추가하기

- 한 텍스트의 단어 비중을 다른 텍스트의 단어 비중으로 나눔

```{r}
frequency_wide <- frequency_wide %>%
  mutate(odds_ratio = ratio_moon/ratio_park)
```

---

- 단어가 어떤 텍스트에서 상대적으로 더 많이 사용됐는지 알 수 있음
  - **`"moon"`에서 상대적인 비중 클수록 1보다 큰 값**
  - `"park"`에서 상대적인 비중 클수록 1보다 작은 값
  - 두 연설문에서 단어 비중 같으면 1

```{r}
frequency_wide %>%
  arrange(-odds_ratio)
```

---

- 단어가 어떤 텍스트에서 상대적으로 더 많이 사용됐는지 알 수 있음
  - `"moon"`에서 상대적인 비중 클수록 1보다 큰 값
  - **`"park"`에서 상대적인 비중 클수록 1보다 작은 값**
  - 두 연설문에서 단어 비중 같으면 1

```{r}
frequency_wide %>%
  arrange(odds_ratio) #<<
```

---

##### 수식으로 표현하면

<br>

.pull-left[

$$\text{odds ratio} = \frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                           {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}$$

<br>

- $n$: 각 단어의 빈도
- $\text{total}$: 전체 단어 빈도

]

---

#### 3. 오즈비 간단히 구하기

```{r}
frequency_wide <- frequency_wide %>%
  mutate(odds_ratio = ((moon + 1)/(sum(moon + 1)))/
                      ((park + 1)/(sum(park + 1))))
```


```{r}
frequency_wide %>%
  arrange(odds_ratio)
```



---

#### 상대적으로 중요한 단어 추출하기

##### 오즈비가 가장 높거나 가장 낮은 단어 추출하기

```{r}
top10 <- frequency_wide %>%
  filter(rank(odds_ratio) <= 10 | rank(-odds_ratio) <= 10)
```

```{r eval=F}
top10 %>%
  arrange(-odds_ratio)
```

---

```{r echo=F}
top10 %>%
  arrange(-odds_ratio)
```

---

- 상위 10개: `"moon"`에서 더 자주 사용되어 `odds_ratio`가 높은 단어

```{r echo=F, highlight.output=c(4:13)}
top10 %>%
  arrange(-odds_ratio)
```

---

- 하위 10개: `"park"`에서 더 자주 사용되어 `odds_ratio`가 낮은 단어

```{r echo=F, highlight.output=c(14:23)}
top10 %>%
  arrange(-odds_ratio)
```

---

#### 막대 그래프 만들기

##### 1. 비중이 큰 연설문을 나타낸 변수 추가하기

```{r eval=F}
top10 <- top10 %>%
  mutate(president = ifelse(odds_ratio > 1, "moon", "park"),
         n = ifelse(odds_ratio > 1, moon, park))

top10
```

```{r echo=F}
top10 <- top10 %>%
  mutate(president = ifelse(odds_ratio > 1, "moon", "park"),
         n = ifelse(odds_ratio > 1, moon, park))
```

---

```{r eval=FALSE}
top10
```

```{r echo=F}
# 문자 길이 무제한 출력 설정
options(tibble.width = Inf)
options(width = 80)
options(pillar.min_chars = Inf)   # 문자 길이
top10
```
---


##### 2. 막대 그래프 만들기

```{r out.width="60%"}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered()
```


```{r}
options(pillar.min_chars = 0)   # 문자 길이 되돌리기
```

---

**전반적으로 "park"의 단어 빈도가 높아보임**
- `"park"`의 `"행복"` 빈도 기준으로 두 그래프의 x축 크기를 똑같이 고정했기 때문


```{r echo=F, out.width="80%"}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered()
```

---

##### 3. 그래프별로 축 설정하기

- 범주별로 단어 비중 알 수 있도록 x축 크기 각각 정하기

```{r out.width="40%"}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free") + #<<
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트
```

---
`r icon_style(fontawesome("exclamation-triangle"), fill = "#FF7333")`  **x축 크기가 그래프마다 다르므로 해석 조심**
- 막대 길이 같아도 단어 빈도 다름
- 두 텍스트 단어 빈도 비교 X
- 각 텍스트에서 상대적으로 중요한 단어가 무엇인지 중심으로 해석

```{r echo=FALSE, out.width="80%"}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free") + #<<
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트
```

---

#### 주요 단어가 사용된 문장 살펴보기

##### 1. 원문을 문장 기준으로 토큰화하기

```{r}
speeches_sentence <- bind_speeches %>%
  as_tibble() %>%
  unnest_tokens(input = value,
                output = sentence,
                token = "sentences")
```


```{r eval=F}
head(speeches_sentence)
tail(speeches_sentence)
```

---

```{r}
head(speeches_sentence)
```


```{r}
tail(speeches_sentence)
```

---


#### 2. 주요 단어가 사용된 문장 추출하기 - `str_detect()`

```{r echo=F}
options(tibble.width = 50)
```

```{r}
speeches_sentence %>%
  filter(president == "moon" & str_detect(sentence, "복지국가"))
```

---

```{r}
speeches_sentence %>%
  filter(president == "park" & str_detect(sentence, "행복"))
```

---

#### 중요도가 비슷한 단어 살펴보기

- `odds_ratio`가 1에 가까운 단어 추출
- 대부분 보편적인 의미를 지니는 단어

```{r}
frequency_wide %>%
  arrange(abs(1 - odds_ratio)) %>%
  head(10)
```

---

##### 중요도가 비슷하면서 빈도가 높은 단어 추출하기

- 두 텍스트에서 모두 강조한 단어

```{r}
frequency_wide %>%
  filter(moon >= 5 & park >= 5) %>%
  arrange(abs(1 - odds_ratio)) %>%
  head(10)
```

---

name: 03-3
class: title1

03-3 로그 오즈비로 단어 비교하기
---
##### 로그 오즈비(log odds ratio)
- 오즈비에 로그를 취한 값
- 단어의 오즈비가 1보다 크면 `+`, 1보다 작으면 `-`가 됨
- 단어가 두 텍스트 중 어디에서 비중이 큰지에 따라 서로 다른 부호
  - `"moon"`에서 비중이 커서 `odds_ratio`가 1보다 큰 단어 `+`
  - `"park"`에서 비중이 커서 `odds_ratio`가 1보다 작은 단어 `-`

<br>

```{r, echo=F, out.width="50%", out.height="50%"}
include_graphics("../Image/etc/03_3_table1.png")
```

---

##### 수식으로 표현하면


<br>
<br>
<br>


$$\text{odds ratio} = \frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                           {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}$$
                           

---

##### 수식으로 표현하면

<br>
<br>
<br>

$$\text{log odds ratio} = \log{\left(\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                              {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right)}$$

---

- 텍스트 차이 분명하게 드러나게 시각화하는데 활용
  - 단어가 어느 텍스트에서 중요한지에 따라 반대되는 축 방향

```{r, echo=F, out.width="60%", out.height="60%"}
include_graphics("../Image/03/03_3_1.png")
```
---


---

<!-- 이번에는 오즈비에 로그를 취한 **로그 오즈비(Log odds ratio)**를 활용하는 방법을 알아보겠습니다. 어떤 값에 로그를 취하면 1보다 큰 값은 양수, 1보다 작은 값은 음수가 됩니다. 따라서 앞에서 구한 오즈비 `odds_ratio`에 로그를 취하면, `moon`에서 비중이 커서 `odds_ratio`가 1보다 큰 단어는 양수가 됩니다. 반대로 `park`에서 비중이 커서 `odds_ratio`가 1보다 작은 단어는 음수가 됩니다. -->

<!-- 로그 오즈비는 단어가 어떤 텍스트에서 비중이 큰지에 따라 서로 다른 부호를 갖습니다. 따라서 로그 오즈비를 이용해 막대 그래프를 만들면 반대되는 축 방향으로 단어의 중요도를 표현해 텍스트의 차이를 분명하게 드러낼 수 있습니다. 단어 빈도 로그 오즈비를 수식으로 나타내면 다음과 같습니다. -->

<!-- $$\text{log odds ratio} = \log{\left(\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}} -->
<!--                               {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right)}$$ -->


<!-- <!-- # 중괄호 --> -->
<!-- <!-- $$\text{log odds ratio} = \log{\left\{\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}} --> -->
<!-- <!--                               {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right\}}$$ --> -->

<!-- <!-- # 괄호 생략                               --> -->
<!-- <!-- $$\text{log odds ratio} = log\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}} --> -->
<!-- <!--                               {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}$$ --> -->






<!-- ### 3.3.1 로그 오즈비 구하기 -->

<!-- 앞에서 구한 `odds_ratio`를 `log()`에 적용하면 로그 오즈비를 구할 수 있습니다. -->
<!-- ```{r} -->
<!-- frequency_wide <- frequency_wide %>% -->
<!--   mutate(log_odds_ratio = log(odds_ratio)) -->
<!-- ``` -->

<!-- `log_odds_ratio`의 부호와 크기를 보면 각 단어가 어떤 연설문에서 더 중요하게 사용됐는지 알 수 있습니다. 단어의 `log_odds_ratio`가 0보다 큰 양수일수록 `"moon"`에서 비중이 크고, 반대로 0보다 작은 음수일수록 `"park"`에서 비중이 크다는 것을 의미합니다. 또한 0에 가까울수록 두 연설문의 비중이 비슷함을 의미합니다. -->

<!-- ```{r} -->
<!-- # moon에서 비중이 큰 단어 -->
<!-- frequency_wide %>% -->
<!--   arrange(-log_odds_ratio) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # park에서 비중이 큰 단어 -->
<!-- frequency_wide %>% -->
<!--   arrange(log_odds_ratio) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # 비중이 비슷한 단어 -->
<!-- frequency_wide %>% -->
<!--   arrange(abs(log_odds_ratio)) -->
<!-- ``` -->


<!-- #### 로그 오즈비 간단히 구하기 -->

<!-- 로그 오즈비 변수만 필요하다면 다음과 같이 간단히 만들 수 있습니다. -->

<!-- ```{r, eval = F} -->
<!-- frequency_wide <- frequency_wide %>% -->
<!--   mutate(log_odds_ratio = log(((moon + 1) / (sum(moon + 1))) / -->
<!--                               ((park + 1) / (sum(park + 1))))) -->
<!-- ``` -->


<!-- ### 3.3.2 상대적으로 중요한 단어 비교하기 -->

<!-- 이제 로그 오즈비 변수를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하겠습니다. 우선 `group_by()`와 `ifelse()`를 이용해 `log_odds_ratio`가 0보다 크면 `"moon"`, 그렇지 않으면 `"park"`을 부여한 `president` 변수를 만들어 항목 별로 분리하겠습니다. 그런 다음, `slice_max()`와 `abs()`를 이용해 `log_odds_ratio`의 절대값 기준으로 상위 10개 단어를 추출하겠습니다. 이렇게 하면 `"moon"`에서 `log_odds_ratio`가 가장 큰 단어 10개, `"park"`에서 `log_odds_ratio`가 가장 작은 단어 10개를 추출합니다. 출력 결과를 보면 **3.2.3**에서 오즈비 `odds_ratio`를 이용해 상하위 10개 단어를 추출했을 때와 같다는 것을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- top10 <- frequency_wide %>% -->
<!--   group_by(president = ifelse(log_odds_ratio > 0, "moon", "park")) %>% -->
<!--   slice_max(abs(log_odds_ratio), n = 10, with_ties = F) -->
<!-- ``` -->


<!-- <!-- ```{r eval=F} --> -->
<!-- <!-- top10 %>%  --> -->
<!-- <!--   arrange(-log_odds_ratio) --> -->
<!-- <!-- ``` --> -->

<!-- <!-- ```{r echo=F} --> -->
<!-- <!-- top10 %>%  --> -->
<!-- <!--   arrange(-log_odds_ratio) %>%  --> -->
<!-- <!--   print(n = 10) --> -->
<!-- <!-- ``` --> -->


<!-- ```{r} -->
<!-- top10 %>%  -->
<!--   arrange(-log_odds_ratio) %>%  -->
<!--   select(word, log_odds_ratio, president) %>%  -->
<!--   print(n = Inf) -->
<!-- ``` -->

<!-- <br> -->

<!-- > [편집] 줄맞춤 -->

<!-- <!-- 이제 로그 오즈비 변수를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하겠습니다. 우선 `filter()`와 `rank()`를 이용해 `log_odds_ratio`의 순위가 상하위 10위에 드는 단어를 추출하겠습니다. 그런 다음, 각 단어가 어떤 연설문에서 더 중요한지 알 수 있도록 `log_odds_ratio`가 0보다 크면 `"moon"`, 그렇지 않으면 `"park"`을 부여한 `president` 변수를 추가하겠습니다. 다음 코드로 추출한 단어를 보면 앞에서 오즈비 `odds_ratio`를 이용해 상하위 10개 단어를 추출했을 때와 같다는 것을 알 수 있습니다. --> -->

<!-- <!-- ```{r} --> -->
<!-- <!-- top10 <- frequency_wide %>% --> -->
<!-- <!--   filter(rank(log_odds_ratio) <= 10 | rank(-log_odds_ratio) <= 10) %>% --> -->
<!-- <!--   mutate(president = ifelse(log_odds_ratio > 0, "moon", "park")) --> -->
<!-- <!-- ``` --> -->


<!-- <!-- ```{r eval=F} --> -->
<!-- <!-- top10 %>% --> -->
<!-- <!--   arrange(-log_odds_ratio) --> -->
<!-- <!-- ``` --> -->


<!-- <!-- ```{r echo=F} --> -->
<!-- <!-- top10 %>% --> -->
<!-- <!--   arrange(-log_odds_ratio) %>% print(n = 10) --> -->
<!-- <!-- ``` --> -->


<!-- ### 3.3.3 막대 그래프 만들기 -->

<!-- 이제 앞에서 만든 데이터를 이용해 막대 그래프를 만들겠습니다. 그래프를 보면 각 단어가 어떤 연설문에서 중요한지에 따라 서로 다른 축 방향으로 표현됩니다. 이처럼 로그 오즈비로 막대 그래프를 만들면 텍스트의 차이를 잘 드러낼 수 있습니다. -->

<!-- ```{r} -->
<!-- ggplot(top10, aes(x = reorder(word, log_odds_ratio), -->
<!--                   y = log_odds_ratio, -->
<!--                   fill = president)) + -->
<!--   geom_col() + -->
<!--   coord_flip() + -->
<!--   labs(x = NULL) + -->
<!--   theme(text = element_text(family = "nanumgothic")) -->
<!-- ``` -->


<!-- ## 3.4 TF-IDF - 여러 텍스트의 단어 비교하기 -->

<!-- 오즈비는 어떤 사건이 두 조건하에서 발생할 확률을 이용해 계산하기 때문에 3개 이상의 텍스트를 비교할 때는 적절하지 않습니다. 텍스트를 둘씩 짝지어 따로따로 비교하는 방법도 있지만 텍스트의 수가 많으면 계산 절차가 길고 결과를 해석하기 어렵기 때문에 효율적이지 않습니다. -->


<!-- #### 중요한 단어란 무엇일까? -->

<!-- 셋 이상의 텍스트를 비교하는 가장 쉬운 방법은 각 텍스트에서 많이 사용된 단어를 알아보는 것입니다. 하지만 앞에서 살펴보았듯이 이런 단어는 누구나 많이 사용하는 흔한 단어이기 때문에 중요하다고 보기 어렵습니다. 예를 들어 대부분의 자기소개서에 "저는"이라는 단어가 많이 나오겠지만 이 단어를 중요하다고 할 수는 없습니다. -->

<!-- 중요한 단어는 '흔하지 않으면서도 특정 텍스트에서는 자주 사용되는 단어'라고 할 수 있습니다. 이런 단어는 특정 텍스트가 다른 텍스트와 구별되는 특징, 개성을 드러냅니다. 예를 들어 어떤 자기소개서에 "스카이다이빙"이라는 흔하지 않은 단어가 여러 번 사용됐다면, 이 단어는 글쓴이의 개성을 잘 드러낸다고 볼 수 있습니다. -->


<!-- #### TF-IDF -->

<!-- **TF-IDF(Term Frequency - Inverse Document Frequency)**는 어떤 단어가 '흔하지 않으면서도 특정 텍스트에서는 자주 사용된 정도'를 나타낸 지표입니다. TF-IDF를 구하면 텍스트의 개성을 드러내는 주요 단어를 찾을 수 있습니다. 계산 과정을 살펴보면서 TF-IDF의 의미를 알아보겠습니다. -->

<!-- > [참고] TF-IDF는 우리말로 '단어 빈도-역문서 빈도'라고 합니다. -->

<!-- #### TF -->

<!-- TF-IDF에서 TF는 텍스트에 단어가 사용된 횟수, **단어 빈도(Term Frequency)**를 의미합니다. 다음 표의 숫자는 자기소개서별로 각 단어가 사용된 횟수, TF를 의미합니다. -->

<!--   ---------------------------------------------------------- -->
<!--    단어          자기소개서 A   자기소개서 B    자기소개서 C -->
<!--   ------------- ------------- -------------  --------------- -->
<!--   저는           15            10             10 -->

<!--   스카이다이빙   3             0              0 -->

<!--   자기주도적     3             5              3 -->

<!--   데이터         0             5              1 -->

<!--   배낭여행       2             3              5 -->
<!--   ---------------------------------------------------------- -->


<!-- #### DF와 IDF -->

<!-- **DF(Document Frequency)**는 단어가 몇 개의 텍스트에 사용됐는지 나타낸 **문서 빈도**입니다. 단어의 DF가 클수록 여러 문서에 흔하게 사용된 일반적인 단어라고 할 수 있습니다. -->

<!-- DF를 이용해 IDF를 구할 수 있습니다. **IDF(Inverse Document Frequency)**는 전체 문서 수(N)에서 DF가 차지하는 비중을 구한 다음, 역수를 취해 로그를 취한 값입니다. 우리 말로는 **역 문서 빈도**라고 합니다. -->

<!-- <!-- $$\text{IDF} = \log{(\frac{{\text{N}}}{{\text{DF}}})}$$ --> -->

<!-- <!-- $$\text{IDF} = \log{\Big(\frac{{\text{N}}}{{\text{DF}}}\Big)}$$ --> -->
<!-- <!-- $$\text{IDF} = \log{\Bigg(\frac{{\text{N}}}{{\text{DF}}}\Bigg)}$$ --> -->

<!-- $$\text{IDF} = \log{\frac{{\text{N}}}{{\text{DF}}}}$$ -->



<!-- IDF는 DF의 역수이므로 DF가 클수록 작아지고 반대로 DF가 작을수록 커집니다. 따라서 IDF가 클수록 드물게 사용되는 특이한 단어, IDF가 작을수록 흔하게 사용되는 일반적인 단어라고 할 수 있습니다. -->

<!-- 다음 표를 보면 "스카이다이빙"을 사용한 자기소개서는 1개 밖에 없기 때문에 IDF가 1.1로 높습니다. 따라서 "스카이다이빙"은 흔하지 않은 단어라고 볼 수 있습니다. 반면 "저는", "자기주도적", "배낭여행"은 모든 자기소개서에 사용되어 IDF가 0이므로 흔한 단어라고 볼 수 있습니다. -->


<!--   ----------------------------------------- -->
<!--    단어           DF             IDF                     -->
<!--   ------------- ------------- ------------- -->
<!--   저는           3             $$log\frac{3}{3} = 0$$  -->

<!--   스카이다이빙   1             $$log\frac{3}{1} = 1.1$$  -->

<!--   자기주도적     3             $$log\frac{3}{3} = 0$$  -->

<!--   데이터         2             $$log\frac{3}{2} = 0.4$$  -->

<!--   배낭여행       3             $$log\frac{3}{3} = 0$$  -->
<!--   ----------------------------------------- -->

<!-- > [참고] 소수점 둘째 자리에서 반올림하여 표기하였습니다. -->

<!-- <br> -->

<!-- > [편집] 수식 줄 맞춤 -->

<!-- <!--   ----------------------------------------- --> -->
<!-- <!--    단어           DF             IDF                     --> -->
<!-- <!--   ------------- ------------- ------------- --> -->
<!-- <!--   저는           3             $$log(\frac{3}{3}) = 0$$  --> -->

<!-- <!--   스카이다이빙   1             $$log(\frac{3}{1}) = 1.1$$  --> -->

<!-- <!--   자기주도적     3             $$log(\frac{3}{3}) = 0$$  --> -->

<!-- <!--   데이터         2             $$log(\frac{3}{2}) = 0.4$$  --> -->

<!-- <!--   배낭여행       3             $$log(\frac{3}{3}) = 0$$  --> -->
<!-- <!--   ----------------------------------------- --> -->


<!-- #### TF-IDF -->

<!-- TF-IDF는 TF(단어 빈도)와 IDF(역문서 빈도)를 곱한 값입니다. TF-IDF는 어떤 단어가 분석 대상이 되는 텍스트 내에서 많이 사용될 수록 커지고(TF), 동시에 해당 단어가 사용된 텍스트가 드물수록 커지는(IDF) 특성을 지닙니다. 즉, '흔하지 않은 단어인데 특정 텍스트에서 자주 사용될수록' 큰 값을 지닙니다. 그러므로 각 텍스트에서 TF-IDF가 큰 단어를 살펴 보면 다른 텍스트와 구별되는 특징을 알 수 있습니다. -->

<!-- <!-- $$\text{TF-IDF} = TF{\times}\log{(\frac{{\text{N}}}{{\text{DF}}})}$$ --> -->

<!-- <!-- $$\text{TF-IDF} = TF{\times}\log{\Big(\frac{{\text{N}}}{{\text{DF}}}\Big)}$$ --> -->

<!-- $$\text{TF-IDF} = TF{\times}\log\frac{{\text{N}}}{{\text{DF}}}$$ -->


<!-- 단어 빈도만 보면 어떤 자기소개서든 가장 많이 사용된 단어는 "저는"입니다. 하지만 다음 표를 보면 자기소개서 A에서 TF-IDF가 가장 높은 단어는 "스카이다이빙"입니다. 따라서 "저는"이 아니라 "스카이다이빙"이 자기소개서 A의 특징을 가장 잘 드러내는 단어라고 할 수 있습니다. -->

<!-- 자기소개서 B와 C는 둘 다 "데이터"의 TF-IDF가 가장 높아 텍스트의 특징을 가장 잘 드러낸다고 할 수 있습니다. 그런데 자기소개서 B는 "데이터"를 5번 사용해 TF-IDF가 2로 높지만, 자기소개서 C는 1번 사용해 TF-IDF가 0.4로 낮습니다. 따라서 "데이터"는 자기소개서 B의 특징을 더 잘 드러내는 단어라고 할 수 있습니다. -->

<!--   <!-- ---------------------------------------------------------------------------------------------------------------------- --> -->
<!--   <!--  단어          자기소개서 A                        자기소개서 B                        자기소개서 C --> -->
<!--   <!-- ------------- ----------                         -------------                      -------------- --> -->
<!--   <!-- 저는           $$15{\times}\log(\frac{3}{3})=0$$   $$10{\times}\log(\frac{3}{3})=0$$   $$10{\times}\log(\frac{3}{3})=0$$ --> -->

<!--   <!-- 스카이다이빙   $$3{\times}\log(\frac{3}{1})=3.3$$  $$0{\times}\log(\frac{3}{1})=0$$    $$0{\times}\log(\frac{3}{1})=0$$ --> -->

<!--   <!-- 자기주도적     $$3{\times}\log(\frac{3}{3})=0$$    $$5{\times}\log(\frac{3}{3})=0$$    $$3{\times}\log(\frac{3}{3})=0$$ --> -->

<!--   <!-- 데이터         $$0{\times}\log(\frac{3}{2})=0$$    $$5{\times}\log(\frac{3}{2})=2$$    $$1{\times}\log(\frac{3}{2})=0.4$$ --> -->

<!--   <!-- 배낭여행       $$2{\times}\log(\frac{3}{3})=0$$    $$3{\times}\log(\frac{3}{3})=0$$    $$5{\times}\log(\frac{3}{3})=0$$ --> -->
<!--   <!-- ---------------------------------------------------------------------------------------------------------------------- --> -->




<!--   ---------------------------------------------------------------------------------------------------------------------- -->
<!--    단어          자기소개서 A                        자기소개서 B                        자기소개서 C -->
<!--   ------------- ----------                        -------------                    -------------- -->
<!--   저는           $$15{\times}\log\frac{3}{3}=0$$   $$10{\times}\log\frac{3}{3}=0$$   $$10{\times}\log\frac{3}{3}=0$$ -->

<!--   스카이다이빙   $$3{\times}\log\frac{3}{1}=3.3$$  $$0{\times}\log\frac{3}{1}=0$$    $$0{\times}\log\frac{3}{1}=0$$ -->

<!--   자기주도적     $$3{\times}\log\frac{3}{3}=0$$    $$5{\times}\log\frac{3}{3}=0$$    $$3{\times}\log\frac{3}{3}=0$$ -->

<!--   데이터         $$0{\times}\log\frac{3}{2}=0$$    $$5{\times}\log\frac{3}{2}=2$$    $$1{\times}\log\frac{3}{2}=0.4$$ -->

<!--   배낭여행       $$2{\times}\log\frac{3}{3}=0$$    $$3{\times}\log\frac{3}{3}=0$$    $$5{\times}\log\frac{3}{3}=0$$ -->
<!--   ---------------------------------------------------------------------------------------------------------------------- -->

<!-- > [참고] 소수점 둘째 자리에서 반올림하여 표기하였습니다. -->

<!-- <br> -->


<!-- > [편집] 각 자기소개서 중 TF-IDF 높은 셀 강조하기, 수식 줄 맞춤 -->


<!-- ### 3.4.1 단어 빈도 구하기 -->

<!-- TF-IDF의 원리를 살펴봤으니 텍스트를 분석하는데 활용해보겠습니다. 먼저 역대 대통령의 대선 출마 선언문을 담은 `speeches_presidents.csv`를 불러와 기본적인 전처리를 한 다음, 명사를 추출해 단어 빈도를 구하겠습니다. CSV 파일을 불러올 때는 `readr` 패키지의 `read_csv()`를 이용하겠습니다. `read_csv()`는 데이터를 다루기 편한 tibble 구조로 만들어주고 `read.csv()`에 비해 속도가 더 빠릅니다. -->


<!-- ```{r echo=F} -->
<!-- options(tibble.width = 47)      # tibble 출력 폭 제한 -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- # 데이터 불러오기 -->
<!-- install.packages("readr") -->
<!-- library(readr) -->

<!-- raw_speeches <- read_csv("speeches_presidents.csv") -->
<!-- raw_speeches -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # install.packages("readr") -->
<!-- library(readr) -->
<!-- raw_speeches <- read_csv(here::here("files/speeches_presidents.csv")) -->
<!-- raw_speeches -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # 기본적인 전처리 -->
<!-- speeches <- raw_speeches %>% -->
<!--   mutate(value = str_replace_all(value, "[^가-힣]", " "), -->
<!--          value = str_squish(value)) -->

<!-- # 토큰화 -->
<!-- speeches <- speeches %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = extractNoun) -->

<!-- # 단어 빈도 구하기 -->
<!-- frequecy <- speeches %>% -->
<!--   count(president, word) %>% -->
<!--   filter(str_count(word) > 1) -->

<!-- frequecy -->
<!-- ``` -->



<!-- ```{r echo=F} -->
<!-- options(tibble.width = 80)      # tibble 출력 폭 제한 -->
<!-- ``` -->

<!-- ### 3.4.2 TF-IDF 구하기 -->

<!-- `tidytext`패키지의 `bind_tf_idf()`를 이용하면 TF-IDF를 구할 수 있습니다. `bind_tf_idf()`에는 세 가지 파라미터를 입력해야 합니다. -->
<!-- - `term`     : 단어 -->
<!-- - `document` : 텍스트 구분 변수 -->
<!-- - `n`        : 단어 빈도 -->

<!-- 앞에서 만든 `frequecy`를 `bind_tf_idf()`에 적용해 TF-IDF를 구한 다음, `tf_idf`가 높은 순으로 정렬하겠습니다. 출력 결과를 보면 `tf`, `idf`, `tf_idf`가 추가되었음을 확인할 수 있습니다. -->

<!-- ```{r} -->
<!-- frequecy <- frequecy %>% -->
<!--   bind_tf_idf(term = word,           # 단어 -->
<!--               document = president,  # 텍스트 구분 변수 -->
<!--               n = n) %>%             # 단어 빈도 -->
<!--   arrange(-tf_idf) -->

<!-- frequecy -->
<!-- ``` -->

<!-- > [참고] bind_tf_idf()로 생성한 `tf`는 앞에서 설명한 tf와 달리 대상 텍스트의 전체 단어 수에서 해당 단어의 수가 차지하는 '비중'을 의미합니다. 단어 빈도를 전체 단어 빈도로 나눈 '비율'이므로 텍스트에 사용된 전체 단어 수가 많을수록 작아집니다. -->


<!-- #### TF-IDF가 높은 단어 살펴보기 -->

<!-- TF-IDF를 구했으니 이제 텍스트의 특징을 드러내는 중요한 단어가 무엇인지 쉽게 파악할 수 있습니다. 살펴볼 텍스트를 추출해 `tf_idf`가 높은 단어를 살펴보면 각 대통령이 다른 대통령들과 달리 무엇을 강조했는지 알 수 있습니다. -->

<!-- ```{r echo=F} -->
<!-- options( -->
<!--   tibble.print_min = 5,  # 행 출력 제한 -->
<!--   tibble.print_max = 5)  # 행 출력 제한 -->
<!-- ``` -->


<!-- ```{r} -->
<!-- frequecy %>% filter(president == "문재인") -->
<!-- ``` -->

<!-- <br> -->

<!-- ```{r} -->
<!-- frequecy %>% filter(president == "박근혜") -->
<!-- ``` -->

<!-- > [편집] 2단 편집 -->

<!-- <br> -->

<!-- ```{r} -->

<!-- frequecy %>% filter(president == "이명박") -->
<!-- ``` -->

<!-- <br> -->

<!-- ```{r} -->

<!-- frequecy %>% filter(president == "노무현") -->
<!-- ``` -->

<!-- > [편집] 2단 편집 -->


<!-- #### TF-IDF가 낮은 단어 살펴보기 -->

<!-- 반대로 TF-IDF가 낮은 단어를 살펴보면, 역대 대통령들이 공통적으로 사용한 흔한 단어를 알 수 있습니다. 출력 결과를 보면 "국민", "경제" 등 범용적인 단어의 TF-IDF가 0임을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- frequecy %>% -->
<!--   filter(president == "문재인") %>% -->
<!--   arrange(tf_idf) -->

<!-- frequecy %>% -->
<!--   filter(president == "박근혜") %>% -->
<!--   arrange(tf_idf) -->
<!-- ``` -->


<!-- ### 3.4.3 막대 그래프 만들기 -->

<!-- 각 연설문에서 TF-IDF가 높은 단어를 추출해 막대 그래프를 만들겠습니다. 출력한 그래프를 보면 각 대통령의 개성을 드러내는 단어가 무엇인지 쉽게 파악할 수 있습니다. -->

<!-- ```{r} -->
<!-- # 주요 단어 추출 -->
<!-- top10 <- frequecy %>% -->
<!--   group_by(president) %>% -->
<!--   slice_max(tf_idf, n = 10, with_ties = F) -->

<!-- # 그래프 순서 정하기 -->
<!-- top10$president <- factor(top10$president, -->
<!--                           levels = c("문재인", "박근혜", "이명박", "노무현")) -->

<!-- # 막대 그래프 만들기 -->
<!-- ggplot(top10, aes(x = reorder_within(word, tf_idf, president), -->
<!--                   y = tf_idf, -->
<!--                   fill = president)) +   -->
<!--   geom_col(show.legend = F) + -->
<!--   coord_flip() + -->
<!--   facet_wrap(~ president, scales = "free", ncol = 2) + -->
<!--   scale_x_reordered() + -->
<!--   labs(x = NULL) + -->
<!--   theme(text = element_text(family = "nanumgothic")) -->
<!-- ``` -->

<!-- > [참고] `factor()`를 이용해 변수 항목의 `levles`를 정하면 원하는 순서로 그래프를 나열할 수 있습니다. -->

<!-- <br> -->

<!-- > [알아두면 좋아요] TF-IDF의 한계와 대안 -->

<!-- 모든 문서에 사용된 단어는 IDF가 0이므로 TF-IDF도 0이 됩니다. 따라서 TF-IDF를 활용하면 단어가 특정 문서에서 특출나게 많이 사용됐더라도 발견할 수 없다는 한계가 있습니다. **Weighted log odds**를 활용하면 이런 한계를 극복할 수 있습니다. Weighted log odds는 단어 등장 확률로 가중치를 부여하기 때문에 모든 문서에 사용됐지만 특정 문서에 많이 사용된 단어를 발견할 수 있습니다. 또한 오즈비와 달리 셋 이상의 문서를 비교할 때도 사용할 수 있다는 장점이 있습니다. `tidylo` 패키지를 이용하면 Weighted log odds를 쉽게 구할 수 있습니다. -->

<!-- - tidylo: Weighted Tidy Log Odds Ratio -->
<!--   [github.com/juliasilge/tidylo](https://github.com/juliasilge/tidylo) -->






<!-- #### 띄어쓰기 기준 토큰화의 문제 -->
<!-- - 의미를 지니지 않는 서술어가 가장 많이 추출됨 -->
<!--     - ex) '합니다', '있습니다' -->

<!-- -- -->

<!-- #### 형태소 분석(Morphological Analysis) -->
<!-- - 문장에서 형태소를 추출해 명사, 동사, 형용사 등 품사로 분류하는 작업 -->
<!-- - 특히 명사를 보고 문장 내용 파악 -->
<!-- - 형태소(Morpheme) -->
<!--   - 의미를 가진 가장 작은 말의 단위 -->
<!--   - 더 나누면 뜻이 없는 문자가 됨 -->


<!-- --- -->

<!-- #### `KoNLP` 한글 형태소 분석 패키지 설치하기 -->

<!-- ##### 1. 자바와 rJava 패키지 설치하기 -->

<!-- ```{r, eval = F} -->
<!-- install.packages("multilinguer") -->
<!-- library(multilinguer) -->
<!-- install_jdk() -->
<!-- ``` -->

<!-- ##### 2. KoNLP 의존성 패키지 설치하기 -->

<!-- ```{r, eval = F} -->
<!-- install.packages(c("stringr", "hash", "tau", "Sejong", "RSQLite", "devtools"), -->
<!--                  type = "binary") -->
<!-- ``` -->

<!-- ##### 3. `KoNLP` 패키지 설치하기 -->
<!-- ```{r, eval=F} -->
<!-- install.packages("remotes") -->
<!-- remotes::install_github("haven-jeon/KoNLP", -->
<!--                         upgrade = "never", -->
<!--                         INSTALL_opts = c("--no-multiarch")) -->

<!-- library(KoNLP) -->
<!-- ``` -->

<!-- --- -->

<!-- #### `KoNLP` 한글 형태소 분석 패키지 설치하기 -->

<!-- ##### 4. 형태소 사전 설정하기 -->

<!-- NIA 사전: 120만여 개 단어로 구성된 형태소 사전 -->

<!-- ```{r, eval = F} -->
<!-- useNIADic() -->
<!-- ``` -->

<!-- `r fontawesome("lightbulb")` `KoNLP` 패키지 설치 후 한 번만 실행 -->

<!-- --- -->

<!-- #### 형태소 분석기를 이용해 토큰화하기 - 명사 추출 -->

<!-- ##### 샘플 텍스트로 작동 원리 알아보기 -->

<!-- ```{r} -->
<!-- library(KoNLP) -->
<!-- library(dplyr) -->

<!-- text <- tibble( -->
<!--   value = c("대한민국은 민주공화국이다.", -->
<!--             "대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")) -->

<!-- text -->
<!-- ``` -->

<!-- --- -->
<!-- `extractNoun()`: 문장에서 추출한 명사를 list 구조로 출력 -->

<!-- ```{r} -->
<!-- extractNoun(text$value) -->
<!-- ``` -->

<!-- `r fontawesome("lightbulb")` `extractNoun()`은 두 번째 실행부터 빠르게 작동 -->

<!-- --- -->

<!-- ##### `unnest_tokens()`를 이용해 명사 추출하기 -->

<!-- - 다루기 쉬운 tibble 구조로 명사 출력 -->

<!-- ```{r} -->
<!-- library(tidytext) -->

<!-- text %>% -->
<!--   unnest_tokens(input = value,        # 분석 대상 -->
<!--                 output = word,        # 출력 변수명 -->
<!--                 token = extractNoun)  # 토큰화 함수  #<< -->
<!-- ``` -->

<!-- `r icon_style(fontawesome("exclamation-triangle"), fill = "#FF7333")` `token` 파라미터에 입력한 `extractNoun` 앞뒤에 따옴표 X -->

<!-- --- -->

<!-- .pull-left[ -->

<!-- ##### 띄어쓰기 기준 추출 -->
<!-- ```{r} -->
<!-- text %>% -->
<!--   unnest_tokens(input = value,     -->
<!--                 output = word,     -->
<!--                 token = "words") #<< -->
<!-- ``` -->

<!-- ] -->

<!-- .pull-right[ -->

<!-- ##### 명사 추출 -->
<!-- ```{r} -->
<!-- text %>% -->
<!--   unnest_tokens(input = value,     -->
<!--                 output = word,     -->
<!--                 token = extractNoun) #<< -->
<!-- ``` -->

<!-- ] -->

<!-- --- -->


<!-- #### 연설문에서 명사 추출하기 -->

<!-- ##### 문재인 대통령 연설문 불러오기 -->

<!-- ```{r eval=F} -->
<!-- raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8") -->
<!-- ``` -->

<!-- ##### 기본적인 전처리 -->
<!-- ```{r eval=F} -->
<!-- library(stringr) -->
<!-- library(textclean) -->

<!-- moon <- raw_moon %>% -->
<!--   str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기 -->
<!--   str_squish() %>%                      # 중복 공백 제거 -->
<!--   as_tibble()                           # tibble로 변환 -->

<!-- moon -->
<!-- ``` -->

<!-- --- -->

<!-- ```{r echo=F} -->
<!-- # 문재인 대통령 연설문 불러오기 -->
<!-- raw_moon <- readLines("../Data/speech_moon.txt", encoding = "UTF-8") -->

<!-- # 기본적인 전처리 -->
<!-- library(stringr) -->
<!-- library(textclean) -->

<!-- moon <- raw_moon %>% -->
<!--   str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기 -->
<!--   str_squish() %>%                      # 중복 공백 제거 -->
<!--   as_tibble()                           # tibble로 변환 -->

<!-- moon -->
<!-- ``` -->

<!-- --- -->

<!-- ##### 명사 기준 토큰화 -->
<!-- ```{r} -->
<!-- word_noun <- moon %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = extractNoun) -->

<!-- word_noun -->
<!-- ``` -->


<!-- --- -->


<!-- name: 02-2 -->
<!-- class: title1 -->

<!-- 02-2 명사 빈도 분석하기 -->

<!-- --- -->

<!-- ##### 단어 빈도 구하기 -->

<!-- - 빈도가 높은 명사를 보면 글쓴이가 무엇을 강조했는지 알 수 있음 -->
<!-- - `# A tibble: 704 x 2`: 연설문이 704개의 명사로 구성됨 -->

<!-- ```{r} -->
<!-- word_noun <- word_noun %>% -->
<!--   count(word, sort = T) %>%    # 단어 빈도 구해 내림차순 정렬 -->
<!--   filter(str_count(word) > 1)  # 두 글자 이상만 남기기 -->

<!-- word_noun -->
<!-- ``` -->

<!-- --- -->

<!-- .pull-left[ -->

<!-- ##### 띄어쓰기 기준 추출 -->
<!-- ```{r} -->
<!-- moon %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = "words") %>% #<< -->
<!--   count(word, sort = T) %>% -->
<!--   filter(str_count(word) > 1) -->

<!-- ``` -->
<!-- ] -->

<!-- .pull-right[ -->

<!-- ##### 명사 추출 -->
<!-- ```{r} -->
<!-- moon %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = extractNoun) %>% #<<  -->
<!--   count(word, sort = T) %>% -->
<!--   filter(str_count(word) > 1) -->

<!-- ``` -->

<!-- ] -->

<!-- --- -->

<!-- ##### 막대 그래프 만들기 -->

<!-- ```{r} -->
<!-- # 상위 20개 단어 추출 -->
<!-- top20 <- word_noun %>% -->
<!--   head(20) -->
<!-- ``` -->

<!-- ```{r eval=F} -->
<!-- # 막대 그래프 만들기 -->
<!-- library(ggplot2) -->

<!-- ggplot(top20, aes(x = reorder(word, n), y = n)) + -->
<!--   geom_col() + -->
<!--   coord_flip() + -->
<!--   geom_text(aes(label = n), hjust = -0.3) + -->
<!--   labs(x = NULL) + -->
<!--   theme(text = element_text(family = "nanumgothic")) -->
<!-- ``` -->

<!-- --- -->

<!-- - 명사로 되어있기 때문에 연설문의 내용을 이해하기 쉬움 -->

<!-- ```{r echo=F, fig.height = 5, out.width = "80%"} -->
<!-- showtext_opts(dpi = 300) # opts_chunk$set(dpi=300) -->

<!-- library(ggplot2) -->
<!-- ggplot(top20, aes(x = reorder(word, n), y = n)) + -->
<!--   geom_col() + -->
<!--   coord_flip() + -->
<!--   geom_text(aes(label = n), hjust = -0.3) + -->
<!--   labs(x = NULL) + -->
<!--   theme(text = element_text(family = "nanumgothic")) -->
<!-- ``` -->

<!-- --- -->

<!-- <br> -->

<!-- .pull-left[ -->

<!-- ##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;띄어쓰기 기준 추출 -->

<!-- ```{r, echo=FALSE} -->
<!-- include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/01/01_3_1.png") -->
<!-- ``` -->
<!-- ] -->

<!-- .pull-right[ -->

<!-- ##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;명사 추출 -->
<!-- ```{r, echo=FALSE} -->
<!-- include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/02/01_2_1.png") -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- ##### 워드 클라우드 만들기 -->

<!-- ```{r eval=F} -->
<!-- # 폰트 설정 -->
<!-- library(showtext) -->
<!-- font_add_google(name = "Black Han Sans", family = "blackhansans") -->
<!-- showtext_auto() -->

<!-- library(ggwordcloud) -->
<!-- ggplot(word_noun, aes(label = word, size = n, col = n)) + -->
<!--   geom_text_wordcloud(seed = 1234, family = "blackhansans") + -->
<!--   scale_radius(limits = c(3, NA), -->
<!--                range = c(3, 15)) + -->
<!--   scale_color_gradient(low = "#66aaf2", high = "#004EA1") + -->
<!--   theme_minimal() -->
<!-- ``` -->

<!-- --- -->
<!-- <br-back-50> -->

<!-- ```{r echo=F} -->
<!-- include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/02/01_2_2.png") -->
<!-- ``` -->

<!-- --- -->

<!-- <br> -->

<!-- .pull-left[ -->

<!-- ##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;띄어쓰기 기준 추출 -->

<!-- <br-back-10> -->

<!-- ```{r, echo=FALSE, out.width="90%", out.height="90%"} -->
<!-- include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/01/01_3_6.png") -->
<!-- ``` -->
<!-- ] -->

<!-- .pull-right[  -->

<!-- ##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;명사 추출 -->

<!-- <br-back-20> -->

<!-- ```{r, echo=FALSE} -->
<!-- include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/Image/02/01_2_2.png") -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- name: 02-3 -->
<!-- class: title1 -->

<!-- 02-3 특정 단어가 사용된 문장 살펴보기 -->

<!-- --- -->

<!-- - 고빈도 단어 사용된 문장 직접 읽어보기 -->
<!-- - 글쓴이가 어떤 의미로 단어를 사용했는지 이해할 수 있음 -->

<!-- -- -->

<!-- ##### 문장 기준으로 토큰화하기 -->

<!-- - 원문 `raw_moon`을 문장 기준으로 토큰화 -->

<!-- ```{r eval=F} -->
<!-- sentences_moon <- raw_moon %>% -->
<!--   str_squish() %>% -->
<!--   as_tibble() %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = sentence, -->
<!--                 token = "sentences") #<< -->

<!-- sentences_moon -->
<!-- ``` -->

<!-- `r icon_style(fontawesome("exclamation-triangle"), fill = "#FF7333")` 문장으로 토큰화할 때는 마침표가 문장의 기준점이 되므로 특수 문자 제거 X -->

<!-- --- -->

<!-- ```{r echo=F} -->
<!-- sentences_moon <- raw_moon %>% -->
<!--   str_squish() %>% -->
<!--   as_tibble() %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = sentence, -->
<!--                 token = "sentences") -->

<!-- sentences_moon -->
<!-- ``` -->

<!-- --- -->

<!-- #### 특정 단어가 사용된 문장 추출하기 -->

<!-- ##### 특정 단어가 들어 있는지 확인하기 - `str_detect()` -->

<!-- - 단어가 문장에 있으면 `TRUE`, 그렇지 않으면 `FALSE` 반환 -->

<!-- ```{r} -->
<!-- str_detect("치킨은 맛있다", "치킨") -->
<!-- str_detect("치킨은 맛있다", "피자") -->
<!-- ``` -->

<!-- --- -->

<!-- ##### 특정 단어가 사용된 문장 추출하기 -->

<!-- .scroll-box-24[ -->

<!-- ```{r} -->
<!-- sentences_moon %>% -->
<!--   filter(str_detect(sentence, "국민")) -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- ##### 특정 단어가 사용된 문장 추출하기 -->

<!-- .scroll-box-24[ -->

<!-- ```{r} -->
<!-- sentences_moon %>% -->
<!--   filter(str_detect(sentence, "일자리")) -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- `r fontawesome("lightbulb")` tibble 구조는 텍스트가 길면 Console 창 크기에 맞춰 일부만 출력함 -->

<!-- - 모든 내용 출력 하려면: `%>% data.frame()` -->
<!-- - 왼쪽 정렬 출력 하려면: `%>% print.data.frame(right = F)` -->

<!-- --- -->

<!-- .box[ -->

<!-- .info[`r icon_style(fontawesome("rocket"), fill = "#FF7333")` 형태소 분석기의 한계] -->

<!-- - 분석 결과에 '하게' 처럼 의미를 알 수 없는 단어가 들어 있음 -->
<!--   - 형태소 사전에 '하게'라는 명사가 있음 -->
<!--   - '당당하게', '절실하게' 등의 '하게'를 명사로 분류해 생긴 오류 -->
<!-- - 형태소 분석기의 성능에 한계가 있기 때문에 분석하면서 오류를 찾아 수정해야 함 -->

<!-- ] -->

<!-- --- -->

<!-- class: title1 -->

<!-- 정리하기 -->

<!-- --- -->

<!-- ### 정리하기 -->

<!-- ##### 1. 명사 추출하기 -->

<!-- ```{r eval=F} -->
<!-- # 명사 기준 토큰화 -->
<!-- word_noun <- moon %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = extractNoun) -->
<!-- ``` -->


<!-- ##### 2. 특정 단어가 사용된 문장 살펴보기 -->
<!-- ```{r eval=F} -->
<!-- # 문장 기준 토큰화 -->
<!-- sentences_moon <- raw_moon %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = sentence, -->
<!--                 token = "sentences") -->

<!-- # 특정 단어가 사용된 문장 추출 -->
<!-- sentences_moon %>% -->
<!--   filter(str_detect(sentence, "국민")) -->
<!-- ``` -->

<!-- --- -->

<!-- class: title1 -->

<!-- 분석 도전 -->

<!-- --- -->

<!-- ### 분석 도전 -->

<!-- 박근혜 전 대통령의 대선 출마 선언문이 들어있는 `speech_park.txt`를 이용해 문제를 해결해 보세요. -->

<!-- Q1. `speech_park.txt`를 불러와 분석에 적합하게 전처리한 다음 연설문에서 명사를 추출하세요. -->

<!-- Q2. 가장 자주 사용된 단어 20개를 추출하세요. -->

<!-- Q3. 가장 자주 사용된 단어 20개의 빈도를 나타낸 막대 그래프를 만드세요. -->

<!-- Q4. 전처리하지 않은 연설문에서 연속된 공백을 제거하고 tibble 구조로 변환한 다음 <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;문장 기준으로 토큰화하세요. -->

<!-- Q5. 연설문에서 `"경제"`가 사용된 문장을 출력하세요. -->

<!-- --- -->

<!-- Q1. `speech_park.txt`를 불러와 분석에 적합하게 전처리한 다음 연설문에서 명사를 추출하세요. -->

<!-- ```{r eval=F} -->
<!-- raw_park <- readLines("speech_park.txt", encoding = "UTF-8") -->

<!-- # 전처리 -->
<!-- library(dplyr) -->
<!-- library(stringr) -->
<!-- park <- raw_park %>% -->
<!--   str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기 -->
<!--   str_squish() %>%                      # 연속된 공백 제거 -->
<!--   as_tibble()                           # tibble로 변환 -->

<!-- park -->
<!-- ``` -->

<!-- --- -->

<!-- Q1. `speech_park.txt`를 불러와 분석에 적합하게 전처리한 다음 연설문에서 명사를 추출하세요. -->

<!-- ```{r echo=F} -->
<!-- raw_park <- readLines(here("Data/speech_park.txt"), encoding = "UTF-8") -->

<!-- # 전처리 -->
<!-- library(dplyr) -->
<!-- library(stringr) -->
<!-- park <- raw_park %>% -->
<!--   str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기 -->
<!--   str_squish() %>%                      # 연속된 공백 제거 -->
<!--   as_tibble()                           # tibble로 변환 -->

<!-- park -->
<!-- ``` -->


<!-- --- -->

<!-- Q1. `speech_park.txt`를 불러와 분석에 적합하게 전처리한 다음 연설문에서 명사를 추출하세요. -->

<!-- .scroll-box-26[ -->
<!-- ```{r} -->
<!-- # 명사 기준 토큰화 -->
<!-- library(tidytext) -->
<!-- library(KoNLP) -->

<!-- word_noun <- park %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = extractNoun) -->

<!-- word_noun -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- Q2. 가장 자주 사용된 단어 20개를 추출하세요. -->

<!-- .scroll-box-26[ -->
<!-- ```{r} -->
<!-- top20 <- word_noun %>% -->
<!--   count(word, sort = T) %>% -->
<!--   filter(str_count(word) > 1) %>% -->
<!--   head(20) -->

<!-- top20 -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- Q3. 가장 자주 사용된 단어 20개의 빈도를 나타낸 막대 그래프를 만드세요. -->
<!-- ```{r eval=F} -->
<!-- library(ggplot2) -->
<!-- ggplot(top20, aes(x = reorder(word, n), y = n)) + -->
<!--   geom_col() + -->
<!--   coord_flip () + -->
<!--   geom_text(aes(label = n), hjust = -0.3) + -->
<!--   labs(x = NULL) -->
<!-- ``` -->

<!-- --- -->

<!-- ```{r echo=F} -->
<!-- library(ggplot2) -->
<!-- ggplot(top20, aes(x = reorder(word, n), y = n)) + -->
<!--   geom_col() + -->
<!--   coord_flip () + -->
<!--   geom_text(aes(label = n), hjust = -0.3) + -->
<!--   labs(x = NULL) -->
<!-- ``` -->

<!-- --- -->

<!-- Q4. 전처리하지 않은 연설문에서 연속된 공백을 제거하고 tibble 구조로 변환한 다음 <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;문장 기준으로 토큰화하세요. -->

<!-- .scroll-box-26[ -->
<!-- ```{r} -->
<!-- sentences_park <- raw_park %>% -->
<!--   str_squish() %>%                    # 연속된 공백 제거 -->
<!--   as_tibble() %>%                     # tibble로 변환 -->
<!--   unnest_tokens(input = value, -->
<!--                 output = sentence, -->
<!--                 token = "sentences") -->

<!-- sentences_park -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- Q5. 연설문에서 `"경제"`가 사용된 문장을 출력하세요. -->

<!-- .scroll-box-26[ -->
<!-- ```{r} -->
<!-- sentences_park %>% -->
<!--   filter(str_detect(sentence, "경제")) -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- class: title0 -->

<!-- 끝 -->
